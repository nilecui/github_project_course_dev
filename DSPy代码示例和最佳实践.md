# DSPyä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µæ¨¡æ¿

## ğŸ“š ä»£ç åº“æ¦‚è§ˆ

æœ¬æ–‡æ¡£æä¾›äº†DSPyå¼€å‘çš„å…¨å¥—ä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µæ¨¡æ¿ï¼ŒåŒ…æ‹¬ï¼š
- ğŸ—ï¸ **åŸºç¡€æ¨¡æ¿**ï¼šå¿«é€Ÿå¼€å§‹çš„æ ‡å‡†æ¨¡æ¿
- ğŸš€ **è¿›é˜¶æ¨¡å¼**ï¼šå¤æ‚åº”ç”¨çš„é«˜çº§æ¨¡å¼
- ğŸ› ï¸ **å·¥å…·ç±»åº“**ï¼šå¸¸ç”¨çš„å·¥å…·å‡½æ•°å’Œç±»
- ğŸ“Š **æœ€ä½³å®è·µ**ï¼šå¼€å‘è§„èŒƒå’Œæ€§èƒ½ä¼˜åŒ–

---

## ğŸ—ï¸ åŸºç¡€æ¨¡æ¿

### 1. DSPyé¡¹ç›®åˆå§‹åŒ–æ¨¡æ¿

#### é¡¹ç›®ç»“æ„
```
dspy_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ logging.conf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_module.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”‚   â””â”€â”€ generation.py
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ rag_pipeline.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ examples/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_modules/
â”‚   â””â”€â”€ test_pipelines/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_getting_started.ipynb
â”‚   â””â”€â”€ 02_advanced_usage.ipynb
â””â”€â”€ scripts/
    â”œâ”€â”€ train.py
    â”œâ”€â”€ evaluate.py
    â””â”€â”€ deploy.py
```

#### requirements.txt
```txt
# DSPyæ ¸å¿ƒ
dspy-ai>=2.0.0

# æœºå™¨å­¦ä¹ 
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0

# ç½‘ç»œè¯·æ±‚
requests>=2.31.0
aiohttp>=3.8.0

# æ•°æ®å¤„ç†
pydantic>=2.0.0
python-dotenv>=1.0.0

# å‘é‡æ•°æ®åº“
chromadb>=0.4.0
faiss-cpu>=1.7.0

# ç›‘æ§å’Œæ—¥å¿—
structlog>=23.0.0
prometheus-client>=0.17.0

# å¼€å‘å·¥å…·
pytest>=7.4.0
black>=23.0.0
ruff>=0.0.280
mypy>=1.5.0

# Jupyteræ”¯æŒ
jupyter>=1.0.0
ipywidgets>=8.0.0
```

#### setup.py
```python
from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="dspy-project",
    version="1.0.0",
    author="Your Name",
    author_email="your.email@example.com",
    description="A DSPy project template",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/dspy-project",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
    ],
    python_requires=">=3.9",
    install_requires=requirements,
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "black>=23.0.0",
            "ruff>=0.0.280",
            "mypy>=1.5.0",
        ],
    },
)
```

#### config/settings.py
```python
from typing import Dict, Any, Optional
import os
from pydantic import BaseSettings, Field
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class DSPySettings(BaseSettings):
    """DSPyåº”ç”¨é…ç½®"""

    # åŸºç¡€é…ç½®
    app_name: str = Field(default="DSPy Application")
    debug: bool = Field(default=False)
    environment: str = Field(default="development")

    # æ¨¡å‹é…ç½®
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")
    default_model: str = Field(default="gpt-3.5-turbo")
    max_tokens: int = Field(default=2000)
    temperature: float = Field(default=0.7)

    # æ•°æ®åº“é…ç½®
    database_url: str = Field(default="sqlite:///./data/dspy.db")

    # ç¼“å­˜é…ç½®
    cache_type: str = Field(default="memory")  # memory, redis, file
    cache_ttl: int = Field(default=3600)  # ç§’

    # æ—¥å¿—é…ç½®
    log_level: str = Field(default="INFO")
    log_file: Optional[str] = Field(default=None)

    # æ€§èƒ½é…ç½®
    max_workers: int = Field(default=4)
    timeout_seconds: int = Field(default=30)

    class Config:
        env_file = ".env"
        case_sensitive = False

class DSPyConfig:
    """DSPyé…ç½®ç®¡ç†å™¨"""

    def __init__(self):
        self.settings = DSPySettings()
        self._configure_dspy()

    def _configure_dspy(self):
        """é…ç½®DSPyå…¨å±€è®¾ç½®"""
        import dspy

        # é…ç½®è¯­è¨€æ¨¡å‹
        lm = dspy.OpenAI(
            model=self.settings.default_model,
            api_key=self.settings.openai_api_key,
            max_tokens=self.settings.max_tokens,
            temperature=self.settings.temperature
        )

        dspy.settings.configure(
            lm=lm,
            rm=None,  # å°†åœ¨éœ€è¦æ—¶é…ç½®æ£€ç´¢å™¨
        )

    def get_config(self) -> Dict[str, Any]:
        """è·å–é…ç½®å­—å…¸"""
        return self.settings.dict()

    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        for key, value in kwargs.items():
            if hasattr(self.settings, key):
                setattr(self.settings, key, value)

        self._configure_dspy()  # é‡æ–°é…ç½®DSPy

# å…¨å±€é…ç½®å®ä¾‹
config = DSPyConfig()
```

#### src/core/base_module.py
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union
import time
import logging
from dataclasses import dataclass

import dspy
from dspy import Example, Prediction

logger = logging.getLogger(__name__)

@dataclass
class ModuleMetrics:
    """æ¨¡å—æ€§èƒ½æŒ‡æ ‡"""
    execution_count: int = 0
    total_time: float = 0.0
    success_count: int = 0
    error_count: int = 0
    avg_time: float = 0.0
    success_rate: float = 0.0

class BaseDSPyModule(dspy.Module, ABC):
    """DSPyæ¨¡å—åŸºç±»"""

    def __init__(self, name: str, **kwargs):
        super().__init__(**kwargs)
        self.name = name
        self.metrics = ModuleMetrics()
        self._start_time = None

    @abstractmethod
    def forward(self, *args, **kwargs) -> Prediction:
        """å­ç±»å¿…é¡»å®ç°çš„å‰å‘ä¼ æ’­æ–¹æ³•"""
        pass

    def __call__(self, *args, **kwargs) -> Prediction:
        """å¸¦æ€§èƒ½ç›‘æ§çš„è°ƒç”¨æ–¹æ³•"""
        self._start_time = time.time()

        try:
            result = self.forward(*args, **kwargs)
            self._record_success()
            return result

        except Exception as e:
            self._record_error(e)
            logger.error(f"æ¨¡å— {self.name} æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def _record_success(self):
        """è®°å½•æˆåŠŸæ‰§è¡Œ"""
        if self._start_time:
            execution_time = time.time() - self._start_time
            self.metrics.execution_count += 1
            self.metrics.total_time += execution_time
            self.metrics.success_count += 1
            self._update_metrics()

    def _record_error(self, error: Exception):
        """è®°å½•æ‰§è¡Œé”™è¯¯"""
        if self._start_time:
            execution_time = time.time() - self._start_time
            self.metrics.execution_count += 1
            self.metrics.total_time += execution_time
            self.metrics.error_count += 1
            self._update_metrics()

    def _update_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        if self.metrics.execution_count > 0:
            self.metrics.avg_time = self.metrics.total_time / self.metrics.execution_count
            self.metrics.success_rate = self.metrics.success_count / self.metrics.execution_count

    def get_metrics(self) -> ModuleMetrics:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics

    def reset_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        self.metrics = ModuleMetrics()

class ConfigurableModule(BaseDSPyModule):
    """å¯é…ç½®çš„DSPyæ¨¡å—"""

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None, **kwargs):
        super().__init__(name, **kwargs)
        self.config = config or {}
        self._load_config()

    def _load_config(self):
        """åŠ è½½é…ç½®"""
        # å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ¥åŠ è½½ç‰¹å®šé…ç½®
        pass

    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.config.update(new_config)
        self._load_config()

    def get_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®"""
        return self.config.copy()

class CachedModule(ConfigurableModule):
    """å¸¦ç¼“å­˜çš„æ¨¡å—"""

    def __init__(self, name: str, cache_size: int = 100, **kwargs):
        super().__init__(name, **kwargs)
        self.cache_size = cache_size
        self.cache: Dict[str, Prediction] = {}
        self.cache_hits = 0
        self.cache_misses = 0

    def _generate_cache_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json

        cache_data = {
            'args': args,
            'kwargs': kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True, default=str)
        return hashlib.md5(cache_str.encode()).hexdigest()

    def __call__(self, *args, **kwargs) -> Prediction:
        """å¸¦ç¼“å­˜çš„è°ƒç”¨"""
        cache_key = self._generate_cache_key(*args, **kwargs)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            self.cache_hits += 1
            logger.debug(f"ç¼“å­˜å‘½ä¸­: {self.name}")
            return self.cache[cache_key]

        # æ‰§è¡Œå¹¶ç¼“å­˜ç»“æœ
        result = super().__call__(*args, **kwargs)

        # ç¼“å­˜ç®¡ç†
        if len(self.cache) >= self.cache_size:
            # ç®€å•çš„LRUï¼šåˆ é™¤ç¬¬ä¸€ä¸ª
            first_key = next(iter(self.cache))
            del self.cache[first_key]

        self.cache[cache_key] = result
        self.cache_misses += 1

        return result

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0

        return {
            'cache_size': len(self.cache),
            'max_cache_size': self.cache_size,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate
        }
```

---

## ğŸš€ è¿›é˜¶æ¨¡å¼

### 2. ä¼ä¸šçº§RAGç³»ç»Ÿæ¨¡æ¿

#### src/pipelines/enterprise_rag.py
```python
from typing import List, Dict, Any, Optional, Union
import asyncio
from dataclasses import dataclass
from enum import Enum

import dspy
from dspy import Example, Prediction

from ..core.base_module import ConfigurableModule, CachedModule
from ..modules.retrieval import HybridRetriever
from ..modules.generation import EnhancedGenerator

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹"""
    FACTUAL = "factual"
    PROCEDURAL = "procedural"
    ANALYTICAL = "analytical"
    CREATIVE = "creative"

@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    query_type: QueryType
    complexity: float  # 0-1
    key_entities: List[str]
    intent: str
    confidence: float

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    documents: List[Dict[str, Any]]
    scores: List[float]
    metadata: Dict[str, Any]

class QueryAnalyzer(CachedModule):
    """æŸ¥è¯¢åˆ†æå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("query_analyzer", config)

        # åˆå§‹åŒ–DSPyç»„ä»¶
        self.classifier = dspy.Predict(ClassifyQueryType)
        self.entity_extractor = dspy.Predict(ExtractEntities)
        self.complexity_analyzer = dspy.Predict(AnalyzeComplexity)

    def forward(self, query: str) -> Prediction:
        """åˆ†ææŸ¥è¯¢"""
        # åˆ†ç±»æŸ¥è¯¢ç±»å‹
        type_result = self.classifier(query=query)

        # æå–å®ä½“
        entity_result = self.entity_extractor(query=query)

        # åˆ†æå¤æ‚åº¦
        complexity_result = self.complexity_analyzer(query=query)

        # æ„å»ºåˆ†æç»“æœ
        analysis = QueryAnalysis(
            query_type=QueryType(type_result.query_type.lower()),
            complexity=complexity_result.complexity_score,
            key_entities=entity_result.entities,
            intent=type_result.intent,
            confidence=type_result.confidence
        )

        return Prediction(
            analysis=analysis,
            query_type=analysis.query_type.value,
            complexity=analysis.complexity,
            entities=analysis.key_entities
        )

class AdaptiveRetriever(ConfigurableModule):
    """è‡ªé€‚åº”æ£€ç´¢å™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("adaptive_retriever", config)

        # åˆå§‹åŒ–å¤šç§æ£€ç´¢å™¨
        self.semantic_retriever = HybridRetriever(
            model_name=self.config.get("semantic_model", "all-MiniLM-L6-v2"),
            index_path=self.config.get("index_path", "./data/semantic_index")
        )

        self.keyword_retriever = HybridRetriever(
            retrieval_type="keyword",
            index_path=self.config.get("keyword_index", "./data/keyword_index")
        )

        # é‡æ’åºå™¨
        self.reranker = dspy.Predict(RerankDocuments)

    def forward(self, query: str, analysis: QueryAnalysis, k: int = 10) -> Prediction:
        """è‡ªé€‚åº”æ£€ç´¢"""
        # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æ£€ç´¢ç­–ç•¥
        if analysis.query_type == QueryType.FACTUAL:
            results = self._factual_retrieval(query, analysis, k)
        elif analysis.query_type == QueryType.ANALYTICAL:
            results = self._analytical_retrieval(query, analysis, k)
        else:
            results = self._hybrid_retrieval(query, analysis, k)

        # é‡æ’åº
        if len(results.documents) > k:
            results = self._rerank_documents(query, results, k)

        return Prediction(
            documents=results.documents[:k],
            scores=results.scores[:k],
            retrieval_strategy=results.metadata.get("strategy", "hybrid")
        )

    def _factual_retrieval(self, query: str, analysis: QueryAnalysis, k: int) -> RetrievalResult:
        """äº‹å®æ€§æ£€ç´¢ - åå‘è¯­ä¹‰æ£€ç´¢"""
        semantic_results = self.semantic_retriever.search(query, k=int(k * 1.5))
        keyword_results = self.keyword_retriever.search(query, k=int(k * 0.5))

        # åˆå¹¶ç»“æœï¼Œè¯­ä¹‰æ£€ç´¢æƒé‡æ›´é«˜
        combined = self._merge_results(
            semantic_results, keyword_results,
            semantic_weight=0.7, keyword_weight=0.3
        )

        combined.metadata["strategy"] = "semantic_weighted"
        return combined

    def _analytical_retrieval(self, query: str, analysis: QueryAnalysis, k: int) -> RetrievalResult:
        """åˆ†ææ€§æ£€ç´¢ - å¹³è¡¡å¤šç§æ£€ç´¢æ–¹å¼"""
        semantic_results = self.semantic_retriever.search(query, k=k)
        keyword_results = self.keyword_retriever.search(query, k=k)

        # åŸºäºå®ä½“æ‰©å±•æŸ¥è¯¢
        expanded_queries = self._expand_with_entities(query, analysis.key_entities)
        expanded_results = []

        for expanded_query in expanded_queries[:3]:
            result = self.semantic_retriever.search(expanded_query, k=int(k * 0.3))
            expanded_results.append(result)

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = [semantic_results, keyword_results] + expanded_results
        combined = self._merge_multiple_results(all_results)

        combined.metadata["strategy"] = "entity_expanded"
        return combined

    def _hybrid_retrieval(self, query: str, analysis: QueryAnalysis, k: int) -> RetrievalResult:
        """æ··åˆæ£€ç´¢"""
        semantic_results = self.semantic_retriever.search(query, k=k)
        keyword_results = self.keyword_retriever.search(query, k=k)

        # å¹³è¡¡æƒé‡
        combined = self._merge_results(
            semantic_results, keyword_results,
            semantic_weight=0.5, keyword_weight=0.5
        )

        combined.metadata["strategy"] = "balanced_hybrid"
        return combined

    def _merge_results(self, semantic_results, keyword_results,
                      semantic_weight: float, keyword_weight: float) -> RetrievalResult:
        """åˆå¹¶ä¸¤ç§æ£€ç´¢ç»“æœ"""
        # ç®€åŒ–çš„åˆå¹¶é€»è¾‘
        all_docs = semantic_results.documents + keyword_results.documents
        all_scores = ([s * semantic_weight for s in semantic_results.scores] +
                     [s * keyword_weight for s in keyword_results.scores])

        # æŒ‰åˆ†æ•°æ’åº
        indexed_results = list(zip(all_docs, all_scores))
        indexed_results.sort(key=lambda x: x[1], reverse=True)

        documents, scores = zip(*indexed_results) if indexed_results else ([], [])

        return RetrievalResult(
            documents=list(documents),
            scores=list(scores),
            metadata={"merged_from": ["semantic", "keyword"]}
        )

    def _expand_with_entities(self, query: str, entities: List[str]) -> List[str]:
        """åŸºäºå®ä½“æ‰©å±•æŸ¥è¯¢"""
        expanded_queries = [query]

        for entity in entities[:3]:  # æœ€å¤šæ‰©å±•3ä¸ªå®ä½“
            # ç®€å•çš„æ‰©å±•ç­–ç•¥
            expanded = f"{query} {entity}"
            expanded_queries.append(expanded)

        return expanded_queries

    def _rerank_documents(self, query: str, results: RetrievalResult, k: int) -> RetrievalResult:
        """é‡æ’åºæ–‡æ¡£"""
        if len(results.documents) <= k:
            return results

        # å‡†å¤‡é‡æ’åºè¾“å…¥
        documents_text = [doc.get("content", str(doc)) for doc in results.documents]

        # è°ƒç”¨é‡æ’åºå™¨
        rerank_result = self.reranker(
            query=query,
            documents=documents_text
        )

        # åº”ç”¨é‡æ’åºç»“æœ
        if hasattr(rerank_result, 'indices') and rerank_result.indices:
            reranked_docs = [results.documents[i] for i in rerank_result.indices[:k]]
            reranked_scores = [rerank_result.scores[i] if hasattr(rerank_result, 'scores')
                             else results.scores[i] for i in rerank_result.indices[:k]]
        else:
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œä¿æŒåŸæ’åº
            reranked_docs = results.documents[:k]
            reranked_scores = results.scores[:k]

        return RetrievalResult(
            documents=reranked_docs,
            scores=reranked_scores,
            metadata={**results.metadata, "reranked": True}
        )

class EnterpriseRAG(ConfigurableModule):
    """ä¼ä¸šçº§RAGç³»ç»Ÿ"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("enterprise_rag", config)

        # åˆå§‹åŒ–ç»„ä»¶
        self.query_analyzer = QueryAnalyzer(self.config.get("query_analyzer", {}))
        self.retriever = AdaptiveRetriever(self.config.get("retriever", {}))
        self.generator = EnhancedGenerator(self.config.get("generator", {}))
        self.evaluator = dspy.Predict(EvaluateAnswer)

        # æ€§èƒ½é…ç½®
        self.max_context_length = self.config.get("max_context_length", 4000)
        self.confidence_threshold = self.config.get("confidence_threshold", 0.6)

    def forward(self, query: str, context_info: Optional[Dict[str, Any]] = None) -> Prediction:
        """å®Œæ•´çš„RAGæµç¨‹"""
        # 1. æŸ¥è¯¢åˆ†æ
        analysis_result = self.query_analyzer(query)
        analysis = analysis_result.analysis

        # 2. è‡ªé€‚åº”æ£€ç´¢
        retrieval_result = self.retriever(query, analysis)

        # 3. ä¸Šä¸‹æ–‡æ„å»º
        context = self._build_context(retrieval_result.documents)

        # 4. ç­”æ¡ˆç”Ÿæˆ
        generation_result = self.generator(
            query=query,
            context=context,
            query_type=analysis.query_type.value,
            complexity=analysis.complexity
        )

        # 5. ç­”æ¡ˆè¯„ä¼°
        evaluation_result = self.evaluator(
            query=query,
            answer=generation_result.answer,
            context=context
        )

        # 6. ç»“æœæ•´åˆ
        final_result = Prediction(
            answer=generation_result.answer,
            confidence=evaluation_result.confidence,
            sources=[doc.get("id", f"doc_{i}") for i, doc in enumerate(retrieval_result.documents)],
            reasoning=generation_result.reasoning,
            query_analysis=analysis,
            retrieval_metadata=retrieval_result.metadata,
            evaluation=evaluation_result.evaluation
        )

        # 7. è´¨é‡æ£€æŸ¥
        if final_result.confidence < self.confidence_threshold:
            final_result.answer += "\n\næ³¨æ„ï¼šæ­¤ç­”æ¡ˆçš„ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®å¯»æ±‚æ›´å¤šæ¥æºéªŒè¯ã€‚"

        return final_result

    def _build_context(self, documents: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []
        current_length = 0

        for doc in documents:
            doc_content = doc.get("content", str(doc))

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_content) > self.max_context_length:
                # æˆªæ–­æ–‡æ¡£
                remaining_space = self.max_context_length - current_length - 50
                if remaining_space > 100:
                    doc_content = doc_content[:remaining_space] + "..."
                else:
                    break

            context_parts.append(f"æ–‡æ¡£{len(context_parts)+1}: {doc_content}")
            current_length += len(doc_content)

        return "\n\n".join(context_parts)

    async def async_forward(self, query: str, context_info: Optional[Dict[str, Any]] = None) -> Prediction:
        """å¼‚æ­¥ç‰ˆæœ¬çš„RAGæµç¨‹"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨å¼‚æ­¥çš„æ£€ç´¢å’Œç”Ÿæˆç»„ä»¶
        loop = asyncio.get_event_loop()

        # å¼‚æ­¥æ‰§è¡Œå„æ­¥éª¤
        analysis_task = loop.run_in_executor(None, self.query_analyzer, query)
        analysis_result = await analysis_task

        retrieval_task = loop.run_in_executor(
            None, self.retriever, query, analysis_result.analysis
        )
        retrieval_result = await retrieval_task

        context = self._build_context(retrieval_result.documents)

        generation_task = loop.run_in_executor(
            None, self.generator, query, context,
            analysis_result.analysis.query_type.value,
            analysis_result.analysis.complexity
        )
        generation_result = await generation_task

        # åç»­æ­¥éª¤...

        return Prediction(answer=generation_result.answer)

# DSPyç­¾åå®šä¹‰
class ClassifyQueryType(dspy.Signature):
    """æŸ¥è¯¢ç±»å‹åˆ†ç±»"""
    query = dspy.InputField(desc="ç”¨æˆ·æŸ¥è¯¢")
    query_type = dspy.OutputField(desc="æŸ¥è¯¢ç±»å‹", choices=["factual", "procedural", "analytical", "creative"])
    intent = dspy.OutputField(desc="ç”¨æˆ·æ„å›¾")
    confidence = dspy.OutputField(desc="åˆ†ç±»ç½®ä¿¡åº¦", type=float)

class ExtractEntities(dspy.Signature):
    """å®ä½“æå–"""
    query = dspy.InputField(desc="ç”¨æˆ·æŸ¥è¯¢")
    entities = dspy.OutputField(desc="å…³é”®å®ä½“åˆ—è¡¨", type=List[str])

class AnalyzeComplexity(dspy.Signature):
    """å¤æ‚åº¦åˆ†æ"""
    query = dspy.InputField(desc="ç”¨æˆ·æŸ¥è¯¢")
    complexity_score = dspy.OutputField(desc="å¤æ‚åº¦è¯„åˆ†(0-1)", type=float)

class RerankDocuments(dspy.Signature):
    """æ–‡æ¡£é‡æ’åº"""
    query = dspy.InputField(desc="æŸ¥è¯¢")
    documents = dspy.InputField(desc="æ–‡æ¡£åˆ—è¡¨", type=List[str])
    indices = dspy.OutputField(desc="æ’åºåçš„ç´¢å¼•", type=List[int])
    scores = dspy.OutputField(desc="é‡æ’åºåˆ†æ•°", type=List[float])

class EvaluateAnswer(dspy.Signature):
    """ç­”æ¡ˆè¯„ä¼°"""
    query = dspy.InputField(desc="åŸå§‹æŸ¥è¯¢")
    answer = dspy.InputField(desc="ç”Ÿæˆçš„ç­”æ¡ˆ")
    context = dspy.InputField(desc="å‚è€ƒä¸Šä¸‹æ–‡")
    evaluation = dspy.OutputField(desc="è¯„ä¼°ç»“æœ")
    confidence = dspy.OutputField(desc="ç­”æ¡ˆç½®ä¿¡åº¦", type=float)
```

### 3. å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæ¨¡æ¿

#### src/modules/multi_agent.py
```python
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

import dspy
from dspy import Example, Prediction

from ..core.base_module import BaseDSPyModule

class AgentRole(Enum):
    """æ™ºèƒ½ä½“è§’è‰²"""
    COORDINATOR = "coordinator"
    RESEARCHER = "researcher"
    ANALYZER = "analyzer"
    WRITER = "writer"
    REVIEWER = "reviewer"
    SPECIALIST = "specialist"

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class AgentTask:
    """æ™ºèƒ½ä½“ä»»åŠ¡"""
    id: str
    agent_id: str
    role: AgentRole
    description: str
    input_data: Dict[str, Any]
    dependencies: List[str] = None
    priority: int = 0
    timeout: Optional[float] = None
    max_retries: int = 3

@dataclass
class TaskResult:
    """ä»»åŠ¡ç»“æœ"""
    task_id: str
    agent_id: str
    status: TaskStatus
    result: Optional[Any] = None
    error: Optional[str] = None
    execution_time: float = 0.0
    retry_count: int = 0

class BaseAgent(BaseDSPyModule):
    """åŸºç¡€æ™ºèƒ½ä½“"""

    def __init__(self, agent_id: str, role: AgentRole, capabilities: List[str] = None, **kwargs):
        super().__init__(f"agent_{agent_id}", **kwargs)
        self.agent_id = agent_id
        self.role = role
        self.capabilities = capabilities or []
        self.current_tasks = {}
        self.task_history = []

    @abstractmethod
    async def execute_task(self, task: AgentTask) -> TaskResult:
        """æ‰§è¡Œä»»åŠ¡"""
        pass

    def can_handle_task(self, task: AgentTask) -> bool:
        """æ£€æŸ¥æ˜¯å¦èƒ½å¤„ç†ä»»åŠ¡"""
        return (task.role == self.role or
                any(cap in task.description.lower() for cap in self.capabilities))

    def get_workload(self) -> int:
        """è·å–å½“å‰å·¥ä½œè´Ÿè½½"""
        return len(self.current_tasks)

class ResearchAgent(BaseAgent):
    """ç ”ç©¶æ™ºèƒ½ä½“"""

    def __init__(self, agent_id: str, **kwargs):
        super().__init__(agent_id, AgentRole.RESEARCHER,
                        capabilities=["research", "search", "analyze"], **kwargs)

        self.retriever = dspy.Retrieve(k=10)
        self.analyzer = dspy.Predict(AnalyzeTopic)

    async def execute_task(self, task: AgentTask) -> TaskResult:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        start_time = time.time()

        try:
            # åˆ†æä¸»é¢˜
            topic_analysis = self.analyzer(topic=task.description)

            # æ£€ç´¢ç›¸å…³ä¿¡æ¯
            search_results = self.retriever(topic_analysis.keywords)

            # æ•´åˆç ”ç©¶ç»“æœ
            research_result = {
                "topic": task.description,
                "analysis": topic_analysis.analysis,
                "findings": search_results.passages,
                "sources": [f"source_{i}" for i in range(len(search_results.passages))],
                "summary": self._generate_summary(search_results.passages)
            }

            execution_time = time.time() - start_time

            return TaskResult(
                task_id=task.id,
                agent_id=self.agent_id,
                status=TaskStatus.COMPLETED,
                result=research_result,
                execution_time=execution_time
            )

        except Exception as e:
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.id,
                agent_id=self.agent_id,
                status=TaskStatus.FAILED,
                error=str(e),
                execution_time=execution_time
            )

    def _generate_summary(self, findings: List[str]) -> str:
        """ç”Ÿæˆç ”ç©¶æ‘˜è¦"""
        if not findings:
            return "æš‚æ— ç›¸å…³ç ”ç©¶å‘ç°"

        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆ
        return f"åŸºäºç ”ç©¶ï¼Œå‘ç°äº†{len(findings)}ä¸ªç›¸å…³ä¿¡æ¯ç‚¹ã€‚ä¸»è¦å‘ç°åŒ…æ‹¬ï¼š{findings[0][:100]}..."

class WritingAgent(BaseAgent):
    """å†™ä½œæ™ºèƒ½ä½“"""

    def __init__(self, agent_id: str, **kwargs):
        super().__init__(agent_id, AgentRole.WRITER,
                        capabilities=["writing", "composition", "editing"], **kwargs)

        self.outliner = dspy.Predict(CreateOutline)
        self.writer = dspy.ChainOfThought(GenerateContent)
        self.editor = dspy.Predict(EditContent)

    async def execute_task(self, task: AgentTask) -> TaskResult:
        """æ‰§è¡Œå†™ä½œä»»åŠ¡"""
        start_time = time.time()

        try:
            research_data = task.input_data.get("research", {})
            topic = task.input_data.get("topic", task.description)

            # åˆ›å»ºå¤§çº²
            outline = self.outliner(topic=topic, research=research_data)

            # ç”Ÿæˆå†…å®¹
            content = self.writer(
                topic=topic,
                outline=outline.outline,
                research=research_data,
                style=task.input_data.get("style", "professional")
            )

            # ç¼–è¾‘ä¼˜åŒ–
            edited_content = self.editor(
                original_content=content.content,
                style=task.input_data.get("style", "professional"),
                target_audience=task.input_data.get("audience", "general")
            )

            writing_result = {
                "topic": topic,
                "outline": outline.outline,
                "content": edited_content.edited_content,
                "word_count": len(edited_content.edited_content.split()),
                "style": task.input_data.get("style", "professional"),
                "sources_used": research_data.get("sources", [])
            }

            execution_time = time.time() - start_time

            return TaskResult(
                task_id=task.id,
                agent_id=self.agent_id,
                status=TaskStatus.COMPLETED,
                result=writing_result,
                execution_time=execution_time
            )

        except Exception as e:
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.id,
                agent_id=self.agent_id,
                status=TaskStatus.FAILED,
                error=str(e),
                execution_time=execution_time
            )

class MultiAgentOrchestrator(BaseDSPyModule):
    """å¤šæ™ºèƒ½ä½“åè°ƒå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, **kwargs):
        super().__init__("multi_agent_orchestrator", **kwargs)
        self.config = config or {}

        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.agents: Dict[str, BaseAgent] = {}
        self.task_queue: List[AgentTask] = []
        self.completed_tasks: Dict[str, TaskResult] = {}

        # ä»»åŠ¡è°ƒåº¦å™¨
        self.scheduler = TaskScheduler(self.config.get("scheduler", {}))

        # æ‰§è¡Œå™¨
        self.max_concurrent_tasks = self.config.get("max_concurrent_tasks", 5)
        self.executor = ThreadPoolExecutor(max_workers=self.max_concurrent_tasks)

    def register_agent(self, agent: BaseAgent):
        """æ³¨å†Œæ™ºèƒ½ä½“"""
        self.agents[agent.agent_id] = agent

    def submit_task(self, task: AgentTask) -> str:
        """æäº¤ä»»åŠ¡"""
        self.task_queue.append(task)
        return task.id

    async def execute_workflow(self, workflow: "Workflow") -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµ"""
        # åˆ›å»ºå·¥ä½œæµä»»åŠ¡
        workflow_tasks = self._create_workflow_tasks(workflow)

        # æäº¤æ‰€æœ‰ä»»åŠ¡
        task_ids = []
        for task in workflow_tasks:
            task_id = self.submit_task(task)
            task_ids.append(task_id)

        # æ‰§è¡Œä»»åŠ¡è°ƒåº¦
        results = await self.scheduler.schedule_and_execute(
            self.task_queue, self.agents, self.executor
        )

        # æ”¶é›†ç»“æœ
        workflow_results = {}
        for task_id in task_ids:
            if task_id in results:
                workflow_results[task_id] = results[task_id]

        return workflow_results

    def _create_workflow_tasks(self, workflow: "Workflow") -> List[AgentTask]:
        """æ ¹æ®å·¥ä½œæµåˆ›å»ºä»»åŠ¡"""
        tasks = []

        for step in workflow.steps:
            task = AgentTask(
                id=f"{workflow.id}_{step.id}",
                agent_id=step.agent_id,
                role=step.role,
                description=step.description,
                input_data=step.input_data,
                dependencies=step.dependencies,
                priority=step.priority
            )
            tasks.append(task)

        return tasks

class TaskScheduler:
    """ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.scheduling_strategy = config.get("strategy", "priority")

    async def schedule_and_execute(self, tasks: List[AgentTask],
                                 agents: Dict[str, BaseAgent],
                                 executor: ThreadPoolExecutor) -> Dict[str, TaskResult]:
        """è°ƒåº¦å’Œæ‰§è¡Œä»»åŠ¡"""
        results = {}
        pending_tasks = tasks.copy()
        running_tasks = {}

        while pending_tasks or running_tasks:
            # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
            completed_task_ids = []
            for task_id, future in running_tasks.items():
                if future.done():
                    try:
                        result = await asyncio.wrap_future(future)
                        results[task_id] = result
                        completed_task_ids.append(task_id)
                    except Exception as e:
                        results[task_id] = TaskResult(
                            task_id=task_id,
                            agent_id="unknown",
                            status=TaskStatus.FAILED,
                            error=str(e)
                        )
                        completed_task_ids.append(task_id)

            # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            for task_id in completed_task_ids:
                del running_tasks[task_id]

            # è°ƒåº¦æ–°ä»»åŠ¡
            scheduled_tasks = self._schedule_tasks(pending_tasks, agents, running_tasks)

            for task in scheduled_tasks:
                agent = agents.get(task.agent_id)
                if agent and agent.can_handle_task(task):
                    future = executor.submit(asyncio.run, agent.execute_task(task))
                    running_tasks[task.id] = future
                    pending_tasks.remove(task)

            # çŸ­æš‚ç­‰å¾…é¿å…å¿™å¾ªç¯
            await asyncio.sleep(0.1)

        return results

    def _schedule_tasks(self, pending_tasks: List[AgentTask],
                       agents: Dict[str, BaseAgent],
                       running_tasks: Dict[str, Any]) -> List[AgentTask]:
        """è°ƒåº¦ä»»åŠ¡"""
        if self.scheduling_strategy == "priority":
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            pending_tasks.sort(key=lambda t: t.priority, reverse=True)

        scheduled_tasks = []

        for task in pending_tasks:
            # æ£€æŸ¥ä¾èµ–
            if self._dependencies_satisfied(task, running_tasks):
                # æ£€æŸ¥ä»£ç†å¯ç”¨æ€§
                agent = agents.get(task.agent_id)
                if agent and agent.get_workload() < 3:  # é™åˆ¶å¹¶å‘ä»»åŠ¡æ•°
                    scheduled_tasks.append(task)

        return scheduled_tasks

    def _dependencies_satisfied(self, task: AgentTask, running_tasks: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ä»»åŠ¡ä¾èµ–æ˜¯å¦æ»¡è¶³"""
        if not task.dependencies:
            return True

        for dep_id in task.dependencies:
            if dep_id in running_tasks:
                return False  # ä¾èµ–ä»»åŠ¡ä»åœ¨è¿è¡Œ

        return True

@dataclass
class WorkflowStep:
    """å·¥ä½œæµæ­¥éª¤"""
    id: str
    agent_id: str
    role: AgentRole
    description: str
    input_data: Dict[str, Any]
    dependencies: List[str] = None
    priority: int = 0

@dataclass
class Workflow:
    """å·¥ä½œæµå®šä¹‰"""
    id: str
    name: str
    description: str
    steps: List[WorkflowStep]

    @classmethod
    def research_and_write(cls, topic: str, style: str = "professional") -> "Workflow":
        """åˆ›å»ºç ”ç©¶å’Œå†™ä½œå·¥ä½œæµ"""
        return Workflow(
            id=f"research_write_{int(time.time())}",
            name=f"ç ”ç©¶å’Œå†™ä½œï¼š{topic}",
            description=f"å¯¹{topic}è¿›è¡Œæ·±å…¥ç ”ç©¶å¹¶æ’°å†™ä¸“ä¸šæ–‡ç« ",
            steps=[
                WorkflowStep(
                    id="research",
                    agent_id="researcher_1",
                    role=AgentRole.RESEARCHER,
                    description=f"ç ”ç©¶{topic}ç›¸å…³ä¿¡æ¯",
                    input_data={"topic": topic},
                    priority=10
                ),
                WorkflowStep(
                    id="writing",
                    agent_id="writer_1",
                    role=AgentRole.WRITER,
                    description=f"æ’°å†™å…³äº{topic}çš„æ–‡ç« ",
                    input_data={"topic": topic, "style": style},
                    dependencies=["research"],
                    priority=8
                )
            ]
        )

# DSPyç­¾åå®šä¹‰
class AnalyzeTopic(dspy.Signature):
    """ä¸»é¢˜åˆ†æ"""
    topic = dspy.InputField(desc="ç ”ç©¶ä¸»é¢˜")
    analysis = dspy.OutputField(desc="ä¸»é¢˜åˆ†æç»“æœ")
    keywords = dspy.OutputField(desc="å…³é”®è¯åˆ—è¡¨", type=List[str])

class CreateOutline(dspy.Signature):
    """åˆ›å»ºå¤§çº²"""
    topic = dspy.InputField(desc="å†™ä½œä¸»é¢˜")
    research = dspy.InputField(desc="ç ”ç©¶èµ„æ–™")
    outline = dspy.OutputField(desc="æ–‡ç« å¤§çº²")

class GenerateContent(dspy.Signature):
    """ç”Ÿæˆå†…å®¹"""
    topic = dspy.InputField(desc="å†™ä½œä¸»é¢˜")
    outline = dspy.InputField(desc="æ–‡ç« å¤§çº²")
    research = dspy.InputField(desc="ç ”ç©¶èµ„æ–™")
    style = dspy.InputField(desc="å†™ä½œé£æ ¼")
    content = dspy.OutputField(desc="ç”Ÿæˆçš„å†…å®¹")

class EditContent(dspy.Signature):
    """ç¼–è¾‘å†…å®¹"""
    original_content = dspy.InputField(desc="åŸå§‹å†…å®¹")
    style = dspy.InputField(desc="ç›®æ ‡é£æ ¼")
    target_audience = dspy.InputField(desc="ç›®æ ‡å—ä¼—")
    edited_content = dspy.OutputField(desc="ç¼–è¾‘åçš„å†…å®¹")
```

---

## ğŸ› ï¸ å·¥å…·ç±»åº“

### 4. æ€§èƒ½ç›‘æ§å·¥å…·

#### src/utils/monitoring.py
```python
import time
import threading
import logging
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from collections import defaultdict, deque
from functools import wraps
import json
from datetime import datetime, timedelta

import psutil
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: float
    execution_time: float
    memory_usage: float
    cpu_usage: float
    success: bool
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, max_history: int = 10000):
        self.max_history = max_history
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_history))
        self.call_counts: Dict[str, int] = defaultdict(int)
        self.error_counts: Dict[str, int] = defaultdict(int)
        self.lock = threading.Lock()

        # ç³»ç»Ÿç›‘æ§
        self.system_monitor = SystemMonitor()
        self.system_monitor.start()

    def record_execution(self, name: str, execution_time: float,
                        success: bool = True, error_message: str = None,
                        metadata: Dict[str, Any] = None):
        """è®°å½•æ‰§è¡ŒæŒ‡æ ‡"""
        with self.lock:
            timestamp = time.time()
            metrics = PerformanceMetrics(
                timestamp=timestamp,
                execution_time=execution_time,
                memory_usage=psutil.Process().memory_info().rss / 1024 / 1024,  # MB
                cpu_usage=psutil.cpu_percent(),
                success=success,
                error_message=error_message,
                metadata=metadata or {}
            )

            self.metrics[name].append(metrics)
            self.call_counts[name] += 1

            if not success:
                self.error_counts[name] += 1

    def get_statistics(self, name: str, time_window: Optional[float] = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            if name not in self.metrics:
                return {}

            metrics_list = list(self.metrics[name])

            # æ—¶é—´çª—å£è¿‡æ»¤
            if time_window:
                cutoff_time = time.time() - time_window
                metrics_list = [m for m in metrics_list if m.timestamp >= cutoff_time]

            if not metrics_list:
                return {}

            execution_times = [m.execution_time for m in metrics_list]
            memory_usages = [m.memory_usage for m in metrics_list]
            success_count = sum(1 for m in metrics_list if m.success)

            stats = {
                'call_count': len(metrics_list),
                'success_count': success_count,
                'error_count': len(metrics_list) - success_count,
                'success_rate': success_count / len(metrics_list),
                'avg_execution_time': np.mean(execution_times),
                'min_execution_time': np.min(execution_times),
                'max_execution_time': np.max(execution_times),
                'p95_execution_time': np.percentile(execution_times, 95),
                'p99_execution_time': np.percentile(execution_times, 99),
                'avg_memory_usage': np.mean(memory_usages),
                'max_memory_usage': np.max(memory_usages),
                'last_execution': metrics_list[-1].timestamp
            }

            return stats

    def get_all_statistics(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        all_stats = {}

        for name in self.metrics.keys():
            all_stats[name] = self.get_statistics(name)

        return all_stats

    def reset_metrics(self, name: Optional[str] = None):
        """é‡ç½®æŒ‡æ ‡"""
        with self.lock:
            if name:
                if name in self.metrics:
                    del self.metrics[name]
                self.call_counts[name] = 0
                self.error_counts[name] = 0
            else:
                self.metrics.clear()
                self.call_counts.clear()
                self.error_counts.clear()

    def export_metrics(self, filename: str):
        """å¯¼å‡ºæŒ‡æ ‡"""
        with self.lock:
            export_data = {
                'timestamp': datetime.now().isoformat(),
                'statistics': self.get_all_statistics(),
                'system_metrics': self.system_monitor.get_current_metrics()
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, interval: float = 1.0):
        self.interval = interval
        self.running = False
        self.thread = None
        self.metrics = deque(maxlen=3600)  # ä¿å­˜1å°æ—¶çš„æ•°æ®

    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.thread.start()

    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.thread:
            self.thread.join()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                metrics = {
                    'timestamp': time.time(),
                    'cpu_percent': psutil.cpu_percent(interval=None),
                    'memory_percent': psutil.virtual_memory().percent,
                    'disk_usage': psutil.disk_usage('/').percent,
                    'process_count': len(psutil.pids()),
                    'network_io': psutil.net_io_counters()._asdict() if psutil.net_io_counters() else {}
                }
                self.metrics.append(metrics)
            except Exception as e:
                logger.error(f"ç³»ç»Ÿç›‘æ§é”™è¯¯: {e}")

            time.sleep(self.interval)

    def get_current_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç³»ç»ŸæŒ‡æ ‡"""
        return self.metrics[-1] if self.metrics else {}

def monitor_performance(name: Optional[str] = None, monitor: Optional[PerformanceMonitor] = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            monitor_instance = monitor or get_default_monitor()
            func_name = name or f"{func.__module__}.{func.__name__}"

            start_time = time.time()
            success = True
            error_message = None

            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                success = False
                error_message = str(e)
                raise
            finally:
                execution_time = time.time() - start_time
                monitor_instance.record_execution(
                    func_name, execution_time, success, error_message
                )

        return wrapper
    return decorator

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_default_monitor = None

def get_default_monitor() -> PerformanceMonitor:
    """è·å–é»˜è®¤ç›‘æ§å™¨"""
    global _default_monitor
    if _default_monitor is None:
        _default_monitor = PerformanceMonitor()
    return _default_monitor

def set_default_monitor(monitor: PerformanceMonitor):
    """è®¾ç½®é»˜è®¤ç›‘æ§å™¨"""
    global _default_monitor
    _default_monitor = monitor
```

### 5. ç¼“å­˜ç®¡ç†å·¥å…·

#### src/utils/caching.py
```python
import hashlib
import json
import pickle
import time
import threading
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, Union, Callable
from dataclasses import dataclass
from pathlib import Path
import sqlite3
import redis
from functools import wraps

logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    timestamp: float
    ttl: Optional[float] = None
    access_count: int = 0
    last_access: float = 0

class CacheBackend(ABC):
    """ç¼“å­˜åç«¯æ¥å£"""

    @abstractmethod
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        pass

    @abstractmethod
    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        pass

    @abstractmethod
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        pass

    @abstractmethod
    def clear(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        pass

    @abstractmethod
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        pass

class MemoryCache(CacheBackend):
    """å†…å­˜ç¼“å­˜"""

    def __init__(self, max_size: int = 1000, default_ttl: Optional[float] = None):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: Dict[str, CacheEntry] = {}
        self.lock = threading.RLock()

    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                return None

            entry = self.cache[key]

            # æ£€æŸ¥TTL
            if self._is_expired(entry):
                del self.cache[key]
                return None

            # æ›´æ–°è®¿é—®ä¿¡æ¯
            entry.access_count += 1
            entry.last_access = time.time()

            return entry.value

    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        with self.lock:
            # æ£€æŸ¥å®¹é‡
            if len(self.cache) >= self.max_size and key not in self.cache:
                self._evict_lru()

            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=time.time(),
                ttl=ttl or self.default_ttl,
                last_access=time.time()
            )

            self.cache[key] = entry
            return True

    def delete(self, key: str) -> bool:
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False

    def clear(self) -> bool:
        with self.lock:
            self.cache.clear()
            return True

    def exists(self, key: str) -> bool:
        with self.lock:
            if key not in self.cache:
                return False

            if self._is_expired(self.cache[key]):
                del self.cache[key]
                return False

            return True

    def _is_expired(self, entry: CacheEntry) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if entry.ttl is None:
            return False
        return time.time() - entry.timestamp > entry.ttl

    def _evict_lru(self):
        """åˆ é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„æ¡ç›®"""
        if not self.cache:
            return

        lru_key = min(
            self.cache.keys(),
            key=lambda k: self.cache[k].last_access
        )
        del self.cache[lru_key]

class FileCache(CacheBackend):
    """æ–‡ä»¶ç¼“å­˜"""

    def __init__(self, cache_dir: str = "./cache", default_ttl: Optional[float] = None):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.default_ttl = default_ttl

    def _get_file_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨MD5é¿å…æ–‡ä»¶åè¿‡é•¿æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦
        safe_key = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{safe_key}.cache"

    def get(self, key: str) -> Optional[Any]:
        file_path = self._get_file_path(key)

        if not file_path.exists():
            return None

        try:
            with open(file_path, 'rb') as f:
                entry = pickle.load(f)

            # æ£€æŸ¥TTL
            if self._is_expired(entry):
                file_path.unlink()
                return None

            return entry.value

        except Exception as e:
            logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        file_path = self._get_file_path(key)

        try:
            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=time.time(),
                ttl=ttl or self.default_ttl,
                last_access=time.time()
            )

            with open(file_path, 'wb') as f:
                pickle.dump(entry, f)

            return True

        except Exception as e:
            logger.error(f"å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def delete(self, key: str) -> bool:
        file_path = self._get_file_path(key)

        try:
            if file_path.exists():
                file_path.unlink()
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def clear(self) -> bool:
        try:
            for file_path in self.cache_dir.glob("*.cache"):
                file_path.unlink()
            return True

        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            return False

    def exists(self, key: str) -> bool:
        file_path = self._get_file_path(key)

        if not file_path.exists():
            return False

        try:
            with open(file_path, 'rb') as f:
                entry = pickle.load(f)

            if self._is_expired(entry):
                file_path.unlink()
                return False

            return True

        except Exception:
            return False

    def _is_expired(self, entry: CacheEntry) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if entry.ttl is None:
            return False
        return time.time() - entry.timestamp > entry.ttl

class RedisCache(CacheBackend):
    """Redisç¼“å­˜"""

    def __init__(self, redis_client, key_prefix: str = "dspy:", default_ttl: Optional[float] = None):
        self.redis = redis_client
        self.key_prefix = key_prefix
        self.default_ttl = default_ttl

    def _make_key(self, key: str) -> str:
        """ç”ŸæˆRedisé”®"""
        return f"{self.key_prefix}{key}"

    def get(self, key: str) -> Optional[Any]:
        try:
            data = self.redis.get(self._make_key(key))
            if data is None:
                return None

            return pickle.loads(data)

        except Exception as e:
            logger.error(f"Redisè¯»å–å¤±è´¥: {e}")
            return None

    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        try:
            data = pickle.dumps(value)
            redis_key = self._make_key(key)
            expire_time = ttl or self.default_ttl

            if expire_time:
                return self.redis.setex(redis_key, int(expire_time), data)
            else:
                return self.redis.set(redis_key, data)

        except Exception as e:
            logger.error(f"Rediså†™å…¥å¤±è´¥: {e}")
            return False

    def delete(self, key: str) -> bool:
        try:
            return bool(self.redis.delete(self._make_key(key)))

        except Exception as e:
            logger.error(f"Redisåˆ é™¤å¤±è´¥: {e}")
            return False

    def clear(self) -> bool:
        try:
            pattern = f"{self.key_prefix}*"
            keys = self.redis.keys(pattern)
            if keys:
                return bool(self.redis.delete(*keys))
            return True

        except Exception as e:
            logger.error(f"Redisæ¸…ç©ºå¤±è´¥: {e}")
            return False

    def exists(self, key: str) -> bool:
        try:
            return bool(self.redis.exists(self._make_key(key)))

        except Exception as e:
            logger.error(f"Redisæ£€æŸ¥å­˜åœ¨å¤±è´¥: {e}")
            return False

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, backend: CacheBackend, key_prefix: str = "dspy"):
        self.backend = backend
        self.key_prefix = key_prefix
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }
        self.lock = threading.Lock()

    def _make_key(self, key_parts: List[Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            'prefix': self.key_prefix,
            'parts': key_parts
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_str.encode()).hexdigest()

    def get(self, *key_parts) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        key = self._make_key(list(key_parts))

        try:
            result = self.backend.get(key)
            if result is not None:
                with self.lock:
                    self.stats['hits'] += 1
                return result
            else:
                with self.lock:
                    self.stats['misses'] += 1
                return None
        except Exception as e:
            logger.error(f"ç¼“å­˜è·å–å¤±è´¥: {e}")
            return None

    def set(self, value: Any, ttl: Optional[float] = None, *key_parts) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        key = self._make_key(list(key_parts))

        try:
            result = self.backend.set(key, value, ttl)
            if result:
                with self.lock:
                    self.stats['sets'] += 1
            return result
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            return False

    def delete(self, *key_parts) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        key = self._make_key(list(key_parts))

        try:
            result = self.backend.delete(key)
            if result:
                with self.lock:
                    self.stats['deletes'] += 1
            return result
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ é™¤å¤±è´¥: {e}")
            return False

    def get_or_set(self, factory: Callable, ttl: Optional[float] = None, *key_parts) -> Any:
        """è·å–æˆ–è®¾ç½®ç¼“å­˜"""
        result = self.get(*key_parts)

        if result is None:
            result = factory()
            self.set(result, ttl, *key_parts)

        return result

    def clear(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        return self.backend.clear()

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return self.stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            for key in self.stats:
                self.stats[key] = 0

def cached(ttl: Optional[float] = None, cache_manager: Optional[CacheManager] = None,
          key_func: Optional[Callable] = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            manager = cache_manager or get_default_cache_manager()

            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                cache_key = [func.__module__, func.__name__] + list(args) + list(sorted(kwargs.items()))

            # å°è¯•è·å–ç¼“å­˜
            result = manager.get(*cache_key)
            if result is not None:
                return result

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            manager.set(result, ttl, *cache_key)

            return result

        return wrapper
    return decorator

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨
_default_cache_manager = None

def get_default_cache_manager() -> CacheManager:
    """è·å–é»˜è®¤ç¼“å­˜ç®¡ç†å™¨"""
    global _default_cache_manager
    if _default_cache_manager is None:
        backend = MemoryCache(max_size=1000, default_ttl=3600)
        _default_cache_manager = CacheManager(backend)
    return _default_cache_manager

def set_default_cache_manager(manager: CacheManager):
    """è®¾ç½®é»˜è®¤ç¼“å­˜ç®¡ç†å™¨"""
    global _default_cache_manager
    _default_cache_manager = manager
```

---

## ğŸ“Š æœ€ä½³å®è·µæŒ‡å—

### 6. å¼€å‘è§„èŒƒå’Œæ€§èƒ½ä¼˜åŒ–

#### å¼€å‘è§„èŒƒ
```python
# src/utils/standards.py

"""
DSPyé¡¹ç›®å¼€å‘è§„èŒƒ
"""

# 1. ä»£ç é£æ ¼è§„èŒƒ
"""
- ä½¿ç”¨Blackè¿›è¡Œä»£ç æ ¼å¼åŒ–
- ä½¿ç”¨Ruffè¿›è¡Œä»£ç æ£€æŸ¥
- ä½¿ç”¨MyPyè¿›è¡Œç±»å‹æ£€æŸ¥
- éµå¾ªPEP 8ç¼–ç è§„èŒƒ
"""

# 2. å‘½åè§„èŒƒ
"""
- ç±»åä½¿ç”¨PascalCaseï¼šEnterpriseRAG, QueryAnalyzer
- å‡½æ•°å’Œå˜é‡ä½¿ç”¨snake_caseï¼šanalyze_query, execution_time
- å¸¸é‡ä½¿ç”¨UPPER_CASEï¼šMAX_TOKENS, DEFAULT_MODEL
- ç§æœ‰æˆå‘˜ä½¿ç”¨ä¸‹åˆ’çº¿å‰ç¼€ï¼š_private_method
"""

# 3. æ–‡æ¡£è§„èŒƒ
"""
- æ‰€æœ‰å…¬å…±å‡½æ•°å¿…é¡»æœ‰docstring
- ä½¿ç”¨Googleé£æ ¼çš„docstring
- åŒ…å«å‚æ•°è¯´æ˜ã€è¿”å›å€¼è¯´æ˜å’Œå¼‚å¸¸è¯´æ˜
"""

def example_function(param1: str, param2: Optional[int] = None) -> Dict[str, Any]:
    """ç¤ºä¾‹å‡½æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²

    Args:
        param1: ç¬¬ä¸€ä¸ªå‚æ•°çš„æè¿°
        param2: ç¬¬äºŒä¸ªå‚æ•°çš„æè¿°ï¼Œå¯é€‰

    Returns:
        åŒ…å«ç»“æœçš„å­—å…¸

    Raises:
        ValueError: å½“å‚æ•°æ— æ•ˆæ—¶
        RuntimeError: å½“è¿è¡Œæ—¶é”™è¯¯æ—¶
    """
    pass

# 4. ç±»å‹æ³¨è§£è§„èŒƒ
"""
- æ‰€æœ‰å‡½æ•°å‚æ•°å’Œè¿”å›å€¼éƒ½è¦æœ‰ç±»å‹æ³¨è§£
- ä½¿ç”¨typingæ¨¡å—ä¸­çš„ç±»å‹
- å¤æ‚ç±»å‹ä½¿ç”¨Unionæˆ–Optional
"""

# 5. é”™è¯¯å¤„ç†è§„èŒƒ
"""
- ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹
- åŒ…å«æœ‰æ„ä¹‰çš„é”™è¯¯æ¶ˆæ¯
- è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
- æä¾›é”™è¯¯æ¢å¤æœºåˆ¶
"""

# 6. æµ‹è¯•è§„èŒƒ
"""
- æ¯ä¸ªæ¨¡å—éƒ½è¦æœ‰å¯¹åº”çš„æµ‹è¯•æ–‡ä»¶
- æµ‹è¯•è¦†ç›–ç‡ä¸ä½äº80%
- ä½¿ç”¨pytestæ¡†æ¶
- åŒ…å«å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
"""
```

#### æ€§èƒ½ä¼˜åŒ–æŒ‡å—
```python
# src/utils/optimization.py

"""
DSPyæ€§èƒ½ä¼˜åŒ–æŒ‡å—
"""

import asyncio
import functools
from typing import Any, Callable, List
import time

# 1. å¼‚æ­¥ä¼˜åŒ–
class AsyncOptimizer:
    """å¼‚æ­¥æ‰§è¡Œä¼˜åŒ–å™¨"""

    @staticmethod
    async def batch_execute(func: Callable, items: List[Any], batch_size: int = 10):
        """æ‰¹é‡å¼‚æ­¥æ‰§è¡Œ"""
        results = []

        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            tasks = [func(item) for item in batch]
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)

        return results

# 2. å†…å­˜ä¼˜åŒ–
class MemoryOptimizer:
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–å™¨"""

    @staticmethod
    def generator_to_list(generator, max_items: int = None):
        """ç”Ÿæˆå™¨è½¬åˆ—è¡¨ï¼Œæ”¯æŒé™åˆ¶æ•°é‡"""
        result = []
        for i, item in enumerate(generator):
            if max_items and i >= max_items:
                break
            result.append(item)
        return result

    @staticmethod
    def clear_cache(obj):
        """æ¸…ç†å¯¹è±¡ç¼“å­˜"""
        if hasattr(obj, '__dict__'):
            obj.__dict__.clear()
        if hasattr(obj, 'cache'):
            obj.cache.clear()

# 3. å¹¶å‘ä¼˜åŒ–
class ConcurrencyOptimizer:
    """å¹¶å‘æ‰§è¡Œä¼˜åŒ–å™¨"""

    @staticmethod
    def parallel_map(func: Callable, items: List[Any], max_workers: int = 4):
        """å¹¶è¡Œæ˜ å°„"""
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(func, items))

        return results

# 4. ç¼“å­˜ä¼˜åŒ–
class CacheOptimizer:
    """ç¼“å­˜ä¼˜åŒ–å™¨"""

    @staticmethod
    def smart_cache(ttl: float = 3600, max_size: int = 1000):
        """æ™ºèƒ½ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            cache = {}
            access_times = {}

            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                key = str(args) + str(sorted(kwargs.items()))
                current_time = time.time()

                # æ£€æŸ¥ç¼“å­˜
                if key in cache:
                    result, timestamp = cache[key]
                    if current_time - timestamp < ttl:
                        access_times[key] = current_time
                        return result
                    else:
                        del cache[key]
                        del access_times[key]

                # æ‰§è¡Œå‡½æ•°
                result = func(*args, **kwargs)

                # ç¼“å­˜ç®¡ç†
                if len(cache) >= max_size:
                    # åˆ é™¤æœ€ä¹…æœªè®¿é—®çš„æ¡ç›®
                    lru_key = min(access_times, key=access_times.get)
                    del cache[lru_key]
                    del access_times[lru_key]

                cache[key] = (result, current_time)
                access_times[key] = current_time

                return result

            return wrapper
        return decorator

# 5. é¢„åŠ è½½ä¼˜åŒ–
class PreloadOptimizer:
    """é¢„åŠ è½½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.preloaded_data = {}

    def preload(self, key: str, loader: Callable):
        """é¢„åŠ è½½æ•°æ®"""
        if key not in self.preloaded_data:
            self.preloaded_data[key] = loader()

    def get(self, key: str):
        """è·å–é¢„åŠ è½½æ•°æ®"""
        return self.preloaded_data.get(key)

# ä½¿ç”¨ç¤ºä¾‹
@CacheOptimizer.smart_cache(ttl=1800, max_size=100)
def expensive_computation(x: int, y: int) -> int:
    """è€—æ—¶çš„è®¡ç®—å‡½æ•°"""
    time.sleep(0.1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return x * y

@functools.lru_cache(maxsize=128)
def memoized_fibonacci(n: int) -> int:
    """è®°å¿†åŒ–æ–æ³¢é‚£å¥‘æ•°åˆ—"""
    if n < 2:
        return n
    return memoized_fibonacci(n-1) + memoized_fibonacci(n-2)
```

### 7. éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ

#### Dockeréƒ¨ç½²æ¨¡æ¿
```dockerfile
# Dockerfile
FROM python:3.10-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æºä»£ç 
COPY src/ ./src/
COPY config/ ./config/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV DSPY_CONFIG_PATH=/app/config

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "src.main"]
```

#### docker-compose.yml
```yaml
version: '3.8'

services:
  dspy-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:password@postgres:5432/dspy
    depends_on:
      - redis
      - postgres
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=dspy
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - dspy-app
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

#### ç›‘æ§é…ç½®
```yaml
# monitoring/docker-compose.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
```

---

è¿™å¥—ä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µæ¨¡æ¿ä¸ºDSPyå¼€å‘æä¾›äº†å®Œæ•´çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š

1. **åŸºç¡€æ¨¡æ¿**ï¼šé¡¹ç›®ç»“æ„ã€é…ç½®ç®¡ç†ã€åŸºç±»è®¾è®¡
2. **è¿›é˜¶æ¨¡å¼**ï¼šä¼ä¸šçº§RAGã€å¤šæ™ºèƒ½ä½“åä½œ
3. **å·¥å…·ç±»åº“**ï¼šæ€§èƒ½ç›‘æ§ã€ç¼“å­˜ç®¡ç†
4. **æœ€ä½³å®è·µ**ï¼šå¼€å‘è§„èŒƒã€æ€§èƒ½ä¼˜åŒ–ã€éƒ¨ç½²è¿ç»´

æ¯ä¸ªæ¨¡æ¿éƒ½ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œå¯ä»¥ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œæˆ–ä½œä¸ºé¡¹ç›®çš„èµ·ç‚¹è¿›è¡Œå®šåˆ¶åŒ–å¼€å‘ã€‚